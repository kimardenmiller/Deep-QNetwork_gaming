Using TensorFlow backend.
[2017-05-11 14:35:15,182] Making new env: CartPole-v0
main
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
flatten_1 (Flatten)              (None, 4)             0           flatten_input_1[0][0]            
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 16)            80          flatten_1[0][0]                  
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 16)            0           dense_1[0][0]                    
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 16)            272         activation_1[0][0]               
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 16)            0           dense_2[0][0]                    
____________________________________________________________________________________________________
dense_3 (Dense)                  (None, 16)            272         activation_2[0][0]               
____________________________________________________________________________________________________
activation_3 (Activation)        (None, 16)            0           dense_3[0][0]                    
____________________________________________________________________________________________________
dense_4 (Dense)                  (None, 2)             34          activation_3[0][0]               
====================================================================================================
Total params: 658
____________________________________________________________________________________________________
None
loading from duel_dqn_CartPole-v0_weights.h5
Training for 50000 steps ...
    29/50000: episode: 1, duration: 0.079s, episode steps: 29, steps per second: 365, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.053 [-0.602, 0.915], loss: --, mean_absolute_error: --, mean_q: --
    43/50000: episode: 2, duration: 0.007s, episode steps: 14, steps per second: 2062, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.049 [-2.057, 1.415], loss: --, mean_absolute_error: --, mean_q: --
    57/50000: episode: 3, duration: 0.538s, episode steps: 14, steps per second: 26, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.079 [-1.205, 0.650], loss: 0.642120, mean_absolute_error: 0.639105, mean_q: 0.200605
    88/50000: episode: 4, duration: 0.110s, episode steps: 31, steps per second: 283, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.100 [-0.415, 1.228], loss: 0.470933, mean_absolute_error: 0.585567, mean_q: 0.446409
   112/50000: episode: 5, duration: 0.086s, episode steps: 24, steps per second: 280, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.064 [-2.679, 1.609], loss: 0.236876, mean_absolute_error: 0.600182, mean_q: 0.816526
   154/50000: episode: 6, duration: 0.155s, episode steps: 42, steps per second: 271, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.026 [-1.165, 1.841], loss: 0.057299, mean_absolute_error: 0.726853, mean_q: 1.321024
   168/50000: episode: 7, duration: 0.052s, episode steps: 14, steps per second: 268, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.094 [-0.940, 1.638], loss: 0.054162, mean_absolute_error: 0.818744, mean_q: 1.510419
   203/50000: episode: 8, duration: 0.126s, episode steps: 35, steps per second: 278, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.105 [-0.593, 1.031], loss: 0.049400, mean_absolute_error: 0.865473, mean_q: 1.592898
   214/50000: episode: 9, duration: 0.042s, episode steps: 11, steps per second: 261, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.107 [-2.254, 1.419], loss: 0.038920, mean_absolute_error: 0.933490, mean_q: 1.747072
   224/50000: episode: 10, duration: 0.038s, episode steps: 10, steps per second: 265, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.116 [-1.585, 2.586], loss: 0.038088, mean_absolute_error: 0.991154, mean_q: 1.903979
   258/50000: episode: 11, duration: 0.120s, episode steps: 34, steps per second: 282, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.618 [0.000, 1.000], mean observation: -0.055 [-2.609, 1.543], loss: 0.054914, mean_absolute_error: 1.094820, mean_q: 2.096227
   310/50000: episode: 12, duration: 0.190s, episode steps: 52, steps per second: 274, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.185 [-0.773, 1.188], loss: 0.078917, mean_absolute_error: 1.300604, mean_q: 2.488452
   320/50000: episode: 13, duration: 0.039s, episode steps: 10, steps per second: 258, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.114 [-2.036, 1.221], loss: 0.080862, mean_absolute_error: 1.401746, mean_q: 2.674274
   332/50000: episode: 14, duration: 0.046s, episode steps: 12, steps per second: 263, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.119 [-1.355, 2.208], loss: 0.107400, mean_absolute_error: 1.478888, mean_q: 2.787249
   342/50000: episode: 15, duration: 0.038s, episode steps: 10, steps per second: 265, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.132 [-2.138, 1.363], loss: 0.060214, mean_absolute_error: 1.527802, mean_q: 2.980357
   355/50000: episode: 16, duration: 0.049s, episode steps: 13, steps per second: 267, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.101 [-1.800, 1.181], loss: 0.161254, mean_absolute_error: 1.622796, mean_q: 3.032942
   375/50000: episode: 17, duration: 0.074s, episode steps: 20, steps per second: 271, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.058 [-1.355, 2.194], loss: 0.128625, mean_absolute_error: 1.659703, mean_q: 3.123842
   390/50000: episode: 18, duration: 0.056s, episode steps: 15, steps per second: 267, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.090 [-1.382, 2.220], loss: 0.117344, mean_absolute_error: 1.713734, mean_q: 3.232428
   407/50000: episode: 19, duration: 0.066s, episode steps: 17, steps per second: 258, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.070 [-1.844, 1.016], loss: 0.108134, mean_absolute_error: 1.765161, mean_q: 3.349638
   422/50000: episode: 20, duration: 0.055s, episode steps: 15, steps per second: 274, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.105 [-0.779, 1.293], loss: 0.139349, mean_absolute_error: 1.863135, mean_q: 3.568868
   457/50000: episode: 21, duration: 0.126s, episode steps: 35, steps per second: 278, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.016 [-1.502, 0.999], loss: 0.133729, mean_absolute_error: 1.967041, mean_q: 3.794209
   481/50000: episode: 22, duration: 0.086s, episode steps: 24, steps per second: 280, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.100 [-1.373, 0.771], loss: 0.165129, mean_absolute_error: 2.090050, mean_q: 3.993866
   495/50000: episode: 23, duration: 0.052s, episode steps: 14, steps per second: 272, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.070 [-1.790, 1.209], loss: 0.151245, mean_absolute_error: 2.169176, mean_q: 4.108184
   510/50000: episode: 24, duration: 0.055s, episode steps: 15, steps per second: 273, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.081 [-1.878, 1.190], loss: 0.177890, mean_absolute_error: 2.234175, mean_q: 4.270784
   535/50000: episode: 25, duration: 0.091s, episode steps: 25, steps per second: 274, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.084 [-1.052, 0.848], loss: 0.185815, mean_absolute_error: 2.307713, mean_q: 4.461055
   561/50000: episode: 26, duration: 0.093s, episode steps: 26, steps per second: 279, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.577 [0.000, 1.000], mean observation: -0.029 [-1.377, 0.983], loss: 0.259852, mean_absolute_error: 2.451033, mean_q: 4.640872
   570/50000: episode: 27, duration: 0.035s, episode steps: 9, steps per second: 255, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.148 [-1.876, 1.164], loss: 0.287719, mean_absolute_error: 2.512936, mean_q: 4.802568
   587/50000: episode: 28, duration: 0.062s, episode steps: 17, steps per second: 273, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.087 [-2.174, 1.323], loss: 0.209780, mean_absolute_error: 2.558180, mean_q: 4.950082
   614/50000: episode: 29, duration: 0.098s, episode steps: 27, steps per second: 275, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.095 [-0.752, 1.560], loss: 0.240287, mean_absolute_error: 2.680820, mean_q: 5.171503
   627/50000: episode: 30, duration: 0.049s, episode steps: 13, steps per second: 267, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.090 [-1.762, 1.215], loss: 0.311662, mean_absolute_error: 2.770241, mean_q: 5.315966
   640/50000: episode: 31, duration: 0.049s, episode steps: 13, steps per second: 267, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.108 [-1.298, 0.754], loss: 0.251741, mean_absolute_error: 2.820507, mean_q: 5.466026
   653/50000: episode: 32, duration: 0.048s, episode steps: 13, steps per second: 269, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.093 [-1.225, 2.024], loss: 0.314476, mean_absolute_error: 2.909744, mean_q: 5.671595
   673/50000: episode: 33, duration: 0.073s, episode steps: 20, steps per second: 273, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.068 [-1.152, 0.620], loss: 0.334242, mean_absolute_error: 2.972542, mean_q: 5.701430
   692/50000: episode: 34, duration: 0.069s, episode steps: 19, steps per second: 275, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.069 [-1.253, 0.835], loss: 0.292721, mean_absolute_error: 3.033297, mean_q: 5.875110
   739/50000: episode: 35, duration: 0.168s, episode steps: 47, steps per second: 280, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.043 [-1.251, 0.763], loss: 0.422171, mean_absolute_error: 3.205680, mean_q: 6.146556
   790/50000: episode: 36, duration: 0.179s, episode steps: 51, steps per second: 285, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.078 [-1.100, 1.059], loss: 0.431741, mean_absolute_error: 3.386122, mean_q: 6.499241
   810/50000: episode: 37, duration: 0.073s, episode steps: 20, steps per second: 273, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.082 [-1.299, 0.619], loss: 0.444025, mean_absolute_error: 3.551328, mean_q: 6.865722
   863/50000: episode: 38, duration: 0.186s, episode steps: 53, steps per second: 284, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.566 [0.000, 1.000], mean observation: 0.040 [-2.092, 1.569], loss: 0.445112, mean_absolute_error: 3.715288, mean_q: 7.218808
   917/50000: episode: 39, duration: 0.191s, episode steps: 54, steps per second: 282, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.135 [-1.385, 1.364], loss: 0.570388, mean_absolute_error: 3.961220, mean_q: 7.680032
   949/50000: episode: 40, duration: 0.114s, episode steps: 32, steps per second: 281, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.067 [-1.306, 0.592], loss: 0.472731, mean_absolute_error: 4.122848, mean_q: 8.120638
   978/50000: episode: 41, duration: 0.105s, episode steps: 29, steps per second: 276, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.072 [-1.144, 0.784], loss: 0.560201, mean_absolute_error: 4.253820, mean_q: 8.365972
   995/50000: episode: 42, duration: 0.064s, episode steps: 17, steps per second: 266, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.096 [-1.186, 0.733], loss: 0.680137, mean_absolute_error: 4.384778, mean_q: 8.571726
  1068/50000: episode: 43, duration: 0.255s, episode steps: 73, steps per second: 286, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.224 [-1.143, 1.726], loss: 0.686632, mean_absolute_error: 4.574537, mean_q: 8.942757
  1107/50000: episode: 44, duration: 0.138s, episode steps: 39, steps per second: 282, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.108 [-0.940, 0.697], loss: 0.615205, mean_absolute_error: 4.816655, mean_q: 9.425832
  1129/50000: episode: 45, duration: 0.080s, episode steps: 22, steps per second: 276, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.100 [-0.564, 1.349], loss: 0.712387, mean_absolute_error: 4.971313, mean_q: 9.678074
  1162/50000: episode: 46, duration: 0.118s, episode steps: 33, steps per second: 279, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.118 [-0.375, 1.084], loss: 0.623512, mean_absolute_error: 5.061639, mean_q: 9.921666
  1175/50000: episode: 47, duration: 0.047s, episode steps: 13, steps per second: 274, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.119 [-0.741, 1.167], loss: 0.756183, mean_absolute_error: 5.120118, mean_q: 9.980259
  1222/50000: episode: 48, duration: 0.167s, episode steps: 47, steps per second: 281, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.426 [0.000, 1.000], mean observation: -0.078 [-1.562, 1.656], loss: 0.728540, mean_absolute_error: 5.301067, mean_q: 10.418066
  1273/50000: episode: 49, duration: 0.180s, episode steps: 51, steps per second: 283, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.148 [-1.468, 0.605], loss: 0.843170, mean_absolute_error: 5.507113, mean_q: 10.799548
  1290/50000: episode: 50, duration: 0.064s, episode steps: 17, steps per second: 267, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.094 [-1.521, 0.956], loss: 1.149814, mean_absolute_error: 5.642689, mean_q: 10.987455
  1379/50000: episode: 51, duration: 0.310s, episode steps: 89, steps per second: 287, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.149 [-1.016, 0.992], loss: 0.805444, mean_absolute_error: 5.859483, mean_q: 11.648781
  1443/50000: episode: 52, duration: 0.224s, episode steps: 64, steps per second: 286, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.124 [-1.004, 1.347], loss: 0.761492, mean_absolute_error: 6.240726, mean_q: 12.415035
  1523/50000: episode: 53, duration: 0.279s, episode steps: 80, steps per second: 286, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.337 [-1.900, 0.902], loss: 0.930978, mean_absolute_error: 6.607900, mean_q: 13.092857
  1650/50000: episode: 54, duration: 0.441s, episode steps: 127, steps per second: 288, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.022 [-1.423, 1.514], loss: 0.903036, mean_absolute_error: 7.086759, mean_q: 14.107770
  1757/50000: episode: 55, duration: 0.372s, episode steps: 107, steps per second: 288, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.157 [-1.435, 0.924], loss: 1.029768, mean_absolute_error: 7.626896, mean_q: 15.257463
  1905/50000: episode: 56, duration: 0.533s, episode steps: 148, steps per second: 278, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.190 [-0.957, 1.286], loss: 0.928238, mean_absolute_error: 8.275088, mean_q: 16.664274
  2026/50000: episode: 57, duration: 0.428s, episode steps: 121, steps per second: 283, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.041 [-1.347, 1.502], loss: 1.030351, mean_absolute_error: 8.908722, mean_q: 17.985029
  2163/50000: episode: 58, duration: 0.484s, episode steps: 137, steps per second: 283, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.239 [-1.622, 0.707], loss: 1.152171, mean_absolute_error: 9.570276, mean_q: 19.414762
  2363/50000: episode: 59, duration: 0.693s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.161 [-1.027, 1.458], loss: 1.359502, mean_absolute_error: 10.448120, mean_q: 21.139074
  2532/50000: episode: 60, duration: 0.589s, episode steps: 169, steps per second: 287, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.186 [-1.507, 0.878], loss: 1.237793, mean_absolute_error: 11.348310, mean_q: 23.000170
  2727/50000: episode: 61, duration: 0.678s, episode steps: 195, steps per second: 288, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.199 [-1.657, 0.850], loss: 1.406175, mean_absolute_error: 12.440655, mean_q: 25.235216
  2871/50000: episode: 62, duration: 0.501s, episode steps: 144, steps per second: 288, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.244 [-1.442, 0.599], loss: 1.366215, mean_absolute_error: 13.315545, mean_q: 27.104845
  3071/50000: episode: 63, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.132 [-1.447, 0.968], loss: 1.601043, mean_absolute_error: 14.341774, mean_q: 29.178091
  3210/50000: episode: 64, duration: 0.483s, episode steps: 139, steps per second: 288, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.243 [-1.597, 0.722], loss: 1.807789, mean_absolute_error: 15.239244, mean_q: 30.962215
  3345/50000: episode: 65, duration: 0.470s, episode steps: 135, steps per second: 287, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.229 [-1.284, 1.126], loss: 1.768703, mean_absolute_error: 16.054848, mean_q: 32.546623
  3498/50000: episode: 66, duration: 0.531s, episode steps: 153, steps per second: 288, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.241 [-1.620, 0.658], loss: 1.777982, mean_absolute_error: 16.844328, mean_q: 34.186398
  3653/50000: episode: 67, duration: 0.539s, episode steps: 155, steps per second: 288, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.233 [-1.656, 0.720], loss: 2.233234, mean_absolute_error: 17.642225, mean_q: 35.798035
  3853/50000: episode: 68, duration: 0.692s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.134 [-1.079, 1.332], loss: 2.161104, mean_absolute_error: 18.452347, mean_q: 37.445175
  4010/50000: episode: 69, duration: 0.544s, episode steps: 157, steps per second: 289, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.247 [-1.635, 0.699], loss: 2.030236, mean_absolute_error: 19.281296, mean_q: 39.283100
  4181/50000: episode: 70, duration: 0.597s, episode steps: 171, steps per second: 287, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.234 [-1.624, 0.719], loss: 2.049311, mean_absolute_error: 20.223167, mean_q: 41.171188
  4363/50000: episode: 71, duration: 0.631s, episode steps: 182, steps per second: 288, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.334 [-1.288, 2.378], loss: 2.542105, mean_absolute_error: 21.033047, mean_q: 42.702206
  4563/50000: episode: 72, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.091 [-0.892, 0.932], loss: 2.345881, mean_absolute_error: 21.906076, mean_q: 44.432064
  4705/50000: episode: 73, duration: 0.496s, episode steps: 142, steps per second: 286, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.245 [-1.506, 0.723], loss: 2.964679, mean_absolute_error: 22.868446, mean_q: 46.265396
  4892/50000: episode: 74, duration: 0.650s, episode steps: 187, steps per second: 288, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.358 [-1.090, 2.404], loss: 2.401410, mean_absolute_error: 23.563946, mean_q: 47.814423
  5092/50000: episode: 75, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.096 [-0.900, 1.334], loss: 3.150194, mean_absolute_error: 24.401098, mean_q: 49.417725
  5292/50000: episode: 76, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.016 [-0.870, 0.824], loss: 3.249266, mean_absolute_error: 25.268633, mean_q: 51.273407
  5468/50000: episode: 77, duration: 0.612s, episode steps: 176, steps per second: 287, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.200 [-1.434, 0.688], loss: 3.774683, mean_absolute_error: 26.212297, mean_q: 53.048618
  5664/50000: episode: 78, duration: 0.682s, episode steps: 196, steps per second: 287, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.276 [-1.531, 2.112], loss: 3.682085, mean_absolute_error: 27.018902, mean_q: 54.717556
  5840/50000: episode: 79, duration: 0.611s, episode steps: 176, steps per second: 288, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.227 [-1.783, 1.145], loss: 2.783103, mean_absolute_error: 27.660168, mean_q: 56.160866
  5978/50000: episode: 80, duration: 0.480s, episode steps: 138, steps per second: 287, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.236 [-1.414, 0.564], loss: 3.304365, mean_absolute_error: 28.499157, mean_q: 57.733513
  6143/50000: episode: 81, duration: 0.572s, episode steps: 165, steps per second: 288, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.221 [-1.813, 0.684], loss: 4.424941, mean_absolute_error: 29.073076, mean_q: 58.899063
  6285/50000: episode: 82, duration: 0.500s, episode steps: 142, steps per second: 284, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.275 [-1.785, 0.583], loss: 4.281052, mean_absolute_error: 29.792257, mean_q: 60.339294
  6431/50000: episode: 83, duration: 0.508s, episode steps: 146, steps per second: 287, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.246 [-1.482, 0.758], loss: 4.250663, mean_absolute_error: 30.299295, mean_q: 61.175495
  6628/50000: episode: 84, duration: 0.684s, episode steps: 197, steps per second: 288, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.210 [-1.960, 0.739], loss: 4.234881, mean_absolute_error: 30.945902, mean_q: 62.638451
  6828/50000: episode: 85, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.244 [-1.072, 2.039], loss: 4.098995, mean_absolute_error: 31.786449, mean_q: 64.305283
  7015/50000: episode: 86, duration: 0.649s, episode steps: 187, steps per second: 288, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.196 [-1.814, 0.729], loss: 3.596554, mean_absolute_error: 32.630367, mean_q: 66.042908
  7182/50000: episode: 87, duration: 0.582s, episode steps: 167, steps per second: 287, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.194 [-1.590, 0.720], loss: 5.382733, mean_absolute_error: 33.318855, mean_q: 67.356636
  7338/50000: episode: 88, duration: 0.542s, episode steps: 156, steps per second: 288, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.256 [-2.042, 0.766], loss: 4.539770, mean_absolute_error: 34.041565, mean_q: 68.791489
  7538/50000: episode: 89, duration: 0.692s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.109 [-1.128, 0.802], loss: 3.648143, mean_absolute_error: 34.569828, mean_q: 69.753204
  7692/50000: episode: 90, duration: 0.536s, episode steps: 154, steps per second: 287, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.235 [-1.789, 0.549], loss: 4.821119, mean_absolute_error: 35.198177, mean_q: 71.119469
  7836/50000: episode: 91, duration: 0.501s, episode steps: 144, steps per second: 288, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.208 [-1.425, 0.658], loss: 4.856188, mean_absolute_error: 35.726948, mean_q: 72.173508
  7999/50000: episode: 92, duration: 0.567s, episode steps: 163, steps per second: 288, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.161 [-1.293, 0.670], loss: 4.727026, mean_absolute_error: 36.193352, mean_q: 73.136070
  8155/50000: episode: 93, duration: 0.543s, episode steps: 156, steps per second: 287, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.229 [-1.828, 0.647], loss: 7.996824, mean_absolute_error: 36.852974, mean_q: 74.297157
  8311/50000: episode: 94, duration: 0.543s, episode steps: 156, steps per second: 287, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.209 [-1.485, 0.801], loss: 5.917404, mean_absolute_error: 37.228951, mean_q: 75.104126
  8511/50000: episode: 95, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.282 [-0.942, 1.992], loss: 5.997062, mean_absolute_error: 37.726177, mean_q: 76.140182
  8710/50000: episode: 96, duration: 0.689s, episode steps: 199, steps per second: 289, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.191 [-1.777, 0.635], loss: 5.487143, mean_absolute_error: 38.279610, mean_q: 77.276863
  8846/50000: episode: 97, duration: 0.473s, episode steps: 136, steps per second: 288, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.267 [-1.838, 0.829], loss: 4.992340, mean_absolute_error: 38.516693, mean_q: 77.960785
  9046/50000: episode: 98, duration: 0.697s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.190 [-1.062, 1.715], loss: 4.738841, mean_absolute_error: 39.320980, mean_q: 79.476479
  9246/50000: episode: 99, duration: 0.692s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.112 [-1.288, 0.604], loss: 5.667880, mean_absolute_error: 39.799030, mean_q: 80.366020
  9376/50000: episode: 100, duration: 0.453s, episode steps: 130, steps per second: 287, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.289 [-2.077, 0.765], loss: 4.806106, mean_absolute_error: 40.377102, mean_q: 81.522804
  9501/50000: episode: 101, duration: 0.438s, episode steps: 125, steps per second: 286, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.243 [-1.630, 0.808], loss: 5.344747, mean_absolute_error: 40.528145, mean_q: 81.785095
  9672/50000: episode: 102, duration: 0.615s, episode steps: 171, steps per second: 278, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.210 [-1.668, 0.549], loss: 4.451489, mean_absolute_error: 41.047314, mean_q: 82.835670
  9866/50000: episode: 103, duration: 0.681s, episode steps: 194, steps per second: 285, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.150 [-1.449, 0.816], loss: 6.205905, mean_absolute_error: 41.609081, mean_q: 83.858452
 10066/50000: episode: 104, duration: 0.701s, episode steps: 200, steps per second: 285, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.075 [-1.111, 0.861], loss: 5.601930, mean_absolute_error: 42.039696, mean_q: 84.747467
 10237/50000: episode: 105, duration: 0.594s, episode steps: 171, steps per second: 288, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.233 [-2.034, 0.806], loss: 7.704979, mean_absolute_error: 42.451241, mean_q: 85.551285
 10407/50000: episode: 106, duration: 0.590s, episode steps: 170, steps per second: 288, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.305 [-2.377, 0.833], loss: 4.992397, mean_absolute_error: 42.935612, mean_q: 86.613831
 10607/50000: episode: 107, duration: 0.696s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.011 [-0.933, 1.050], loss: 5.625962, mean_absolute_error: 43.410290, mean_q: 87.589378
 10801/50000: episode: 108, duration: 0.675s, episode steps: 194, steps per second: 287, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.356 [-0.685, 2.411], loss: 8.222630, mean_absolute_error: 43.843910, mean_q: 88.235466
 11001/50000: episode: 109, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.048 [-0.791, 0.841], loss: 6.391439, mean_absolute_error: 44.286011, mean_q: 89.244865
 11154/50000: episode: 110, duration: 0.534s, episode steps: 153, steps per second: 287, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.328 [-2.367, 0.651], loss: 7.222424, mean_absolute_error: 44.519112, mean_q: 89.817825
 11314/50000: episode: 111, duration: 0.557s, episode steps: 160, steps per second: 287, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.227 [-1.652, 0.904], loss: 8.230115, mean_absolute_error: 44.662228, mean_q: 90.001358
 11494/50000: episode: 112, duration: 0.625s, episode steps: 180, steps per second: 288, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.210 [-1.817, 0.765], loss: 7.563228, mean_absolute_error: 45.049103, mean_q: 90.720207
 11694/50000: episode: 113, duration: 0.693s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.337 [-0.763, 2.404], loss: 5.915362, mean_absolute_error: 45.574387, mean_q: 91.804726
 11878/50000: episode: 114, duration: 0.639s, episode steps: 184, steps per second: 288, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.277 [-2.229, 0.782], loss: 7.019664, mean_absolute_error: 45.526283, mean_q: 91.710930
 12062/50000: episode: 115, duration: 0.678s, episode steps: 184, steps per second: 271, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.210 [-2.062, 0.992], loss: 6.577107, mean_absolute_error: 46.091118, mean_q: 92.614006
 12260/50000: episode: 116, duration: 0.689s, episode steps: 198, steps per second: 288, episode reward: 198.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.335 [-0.554, 2.416], loss: 4.302173, mean_absolute_error: 46.191833, mean_q: 93.001991
 12418/50000: episode: 117, duration: 0.549s, episode steps: 158, steps per second: 288, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.230 [-1.649, 0.994], loss: 5.547804, mean_absolute_error: 46.606255, mean_q: 93.776665
 12574/50000: episode: 118, duration: 0.543s, episode steps: 156, steps per second: 288, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.279 [-2.248, 0.908], loss: 8.116533, mean_absolute_error: 46.532043, mean_q: 93.684723
 12699/50000: episode: 119, duration: 0.435s, episode steps: 125, steps per second: 287, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.299 [-2.003, 0.763], loss: 9.031183, mean_absolute_error: 46.814564, mean_q: 94.130348
 12832/50000: episode: 120, duration: 0.462s, episode steps: 133, steps per second: 288, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.272 [-1.618, 1.290], loss: 4.302605, mean_absolute_error: 46.731197, mean_q: 94.113068
 12953/50000: episode: 121, duration: 0.421s, episode steps: 121, steps per second: 287, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.292 [-1.697, 0.682], loss: 5.250242, mean_absolute_error: 47.120224, mean_q: 94.758827
 13153/50000: episode: 122, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.279 [-0.854, 2.011], loss: 7.049887, mean_absolute_error: 47.396019, mean_q: 95.431839
 13279/50000: episode: 123, duration: 0.440s, episode steps: 126, steps per second: 286, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.310 [-1.891, 0.839], loss: 6.454618, mean_absolute_error: 47.631687, mean_q: 95.726776
 13479/50000: episode: 124, duration: 0.697s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.154 [-1.650, 0.813], loss: 5.738143, mean_absolute_error: 47.554832, mean_q: 95.689568
 13649/50000: episode: 125, duration: 0.592s, episode steps: 170, steps per second: 287, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.229 [-1.854, 0.872], loss: 8.105306, mean_absolute_error: 47.649494, mean_q: 95.787415
 13786/50000: episode: 126, duration: 0.477s, episode steps: 137, steps per second: 287, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.270 [-1.797, 0.890], loss: 5.363132, mean_absolute_error: 48.002357, mean_q: 96.449013
 13950/50000: episode: 127, duration: 0.571s, episode steps: 164, steps per second: 287, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.253 [-1.782, 1.214], loss: 10.345427, mean_absolute_error: 48.102341, mean_q: 96.595665
 14150/50000: episode: 128, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.125 [-1.440, 0.913], loss: 7.500546, mean_absolute_error: 48.381729, mean_q: 97.308929
 14337/50000: episode: 129, duration: 0.651s, episode steps: 187, steps per second: 287, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.245 [-2.027, 0.790], loss: 6.001243, mean_absolute_error: 48.275749, mean_q: 97.232956
 14537/50000: episode: 130, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.031 [-1.071, 0.821], loss: 11.254661, mean_absolute_error: 48.682911, mean_q: 97.703880
 14675/50000: episode: 131, duration: 0.480s, episode steps: 138, steps per second: 288, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.282 [-2.027, 0.843], loss: 9.360898, mean_absolute_error: 48.801472, mean_q: 97.907593
 14875/50000: episode: 132, duration: 0.693s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.333 [-0.767, 2.403], loss: 5.227085, mean_absolute_error: 48.923290, mean_q: 98.327705
 15075/50000: episode: 133, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.173 [-1.031, 1.517], loss: 3.638206, mean_absolute_error: 49.183487, mean_q: 98.916550
 15236/50000: episode: 134, duration: 0.561s, episode steps: 161, steps per second: 287, episode reward: 161.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.273 [-2.006, 0.809], loss: 6.473301, mean_absolute_error: 49.531559, mean_q: 99.478119
 15407/50000: episode: 135, duration: 0.596s, episode steps: 171, steps per second: 287, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.237 [-1.853, 1.207], loss: 6.724697, mean_absolute_error: 49.487492, mean_q: 99.354797
 15607/50000: episode: 136, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.023 [-0.930, 0.965], loss: 10.584031, mean_absolute_error: 49.402187, mean_q: 99.131905
 15782/50000: episode: 137, duration: 0.607s, episode steps: 175, steps per second: 288, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.188 [-1.674, 1.070], loss: 6.599730, mean_absolute_error: 49.642200, mean_q: 99.595825
 15972/50000: episode: 138, duration: 0.660s, episode steps: 190, steps per second: 288, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.182 [-1.837, 1.057], loss: 7.937731, mean_absolute_error: 49.663704, mean_q: 99.696648
 16172/50000: episode: 139, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.049 [-0.767, 0.976], loss: 3.667855, mean_absolute_error: 49.939930, mean_q: 100.448128
 16319/50000: episode: 140, duration: 0.512s, episode steps: 147, steps per second: 287, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.277 [-2.050, 1.197], loss: 6.885846, mean_absolute_error: 50.092754, mean_q: 100.811089
 16497/50000: episode: 141, duration: 0.618s, episode steps: 178, steps per second: 288, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.184 [-1.707, 1.095], loss: 6.879636, mean_absolute_error: 50.247948, mean_q: 101.050980
 16643/50000: episode: 142, duration: 0.508s, episode steps: 146, steps per second: 288, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.251 [-1.492, 1.129], loss: 4.902254, mean_absolute_error: 50.335541, mean_q: 101.079010
 16810/50000: episode: 143, duration: 0.580s, episode steps: 167, steps per second: 288, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.406 [-0.860, 2.421], loss: 7.728204, mean_absolute_error: 50.298008, mean_q: 101.008827
 17010/50000: episode: 144, duration: 0.696s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.136 [-1.513, 1.188], loss: 6.520294, mean_absolute_error: 50.617100, mean_q: 101.707314
 17210/50000: episode: 145, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.279 [-0.810, 1.993], loss: 9.470241, mean_absolute_error: 50.575390, mean_q: 101.671425
 17327/50000: episode: 146, duration: 0.408s, episode steps: 117, steps per second: 287, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.248 [-1.521, 0.991], loss: 6.747694, mean_absolute_error: 50.467861, mean_q: 101.473465
 17484/50000: episode: 147, duration: 0.545s, episode steps: 157, steps per second: 288, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.253 [-1.723, 1.145], loss: 7.845972, mean_absolute_error: 50.420498, mean_q: 101.372086
 17684/50000: episode: 148, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.187 [-0.808, 1.440], loss: 8.254425, mean_absolute_error: 50.592842, mean_q: 101.600334
 17884/50000: episode: 149, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.046 [-0.808, 1.061], loss: 7.597106, mean_absolute_error: 50.910145, mean_q: 102.280823
 18003/50000: episode: 150, duration: 0.414s, episode steps: 119, steps per second: 287, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.209 [-1.513, 1.153], loss: 9.155410, mean_absolute_error: 51.079662, mean_q: 102.432510
 18203/50000: episode: 151, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.176 [-1.120, 1.323], loss: 6.736543, mean_absolute_error: 51.094044, mean_q: 102.606964
 18349/50000: episode: 152, duration: 0.508s, episode steps: 146, steps per second: 287, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.266 [-1.887, 1.267], loss: 7.861938, mean_absolute_error: 51.461689, mean_q: 103.123352
 18529/50000: episode: 153, duration: 0.625s, episode steps: 180, steps per second: 288, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.202 [-1.646, 1.123], loss: 7.249597, mean_absolute_error: 51.230740, mean_q: 102.807289
 18729/50000: episode: 154, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.027 [-1.308, 1.207], loss: 3.022990, mean_absolute_error: 51.522999, mean_q: 103.731407
 18929/50000: episode: 155, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.309 [-0.975, 2.155], loss: 4.227432, mean_absolute_error: 51.850754, mean_q: 104.402008
 19115/50000: episode: 156, duration: 0.647s, episode steps: 186, steps per second: 288, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.169 [-1.895, 1.277], loss: 8.473694, mean_absolute_error: 51.981937, mean_q: 104.504166
 19315/50000: episode: 157, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.074 [-1.401, 1.233], loss: 5.333365, mean_absolute_error: 51.955688, mean_q: 104.493896
 19482/50000: episode: 158, duration: 0.582s, episode steps: 167, steps per second: 287, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.237 [-1.834, 1.254], loss: 6.957605, mean_absolute_error: 52.323875, mean_q: 105.112999
 19682/50000: episode: 159, duration: 0.693s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.182 [-1.153, 1.335], loss: 9.790676, mean_absolute_error: 52.705204, mean_q: 105.804886
 19810/50000: episode: 160, duration: 0.446s, episode steps: 128, steps per second: 287, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.217 [-1.641, 1.220], loss: 7.924188, mean_absolute_error: 52.730934, mean_q: 105.986443
 19994/50000: episode: 161, duration: 0.640s, episode steps: 184, steps per second: 288, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.372 [-0.952, 2.411], loss: 13.036661, mean_absolute_error: 52.588703, mean_q: 105.500298
 20190/50000: episode: 162, duration: 0.680s, episode steps: 196, steps per second: 288, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.351 [-1.053, 2.421], loss: 6.651697, mean_absolute_error: 52.616859, mean_q: 105.804970
 20381/50000: episode: 163, duration: 0.662s, episode steps: 191, steps per second: 288, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.206 [-1.718, 1.053], loss: 6.740256, mean_absolute_error: 52.648304, mean_q: 105.836021
 20581/50000: episode: 164, duration: 0.693s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.335 [-1.038, 2.254], loss: 5.131247, mean_absolute_error: 52.983009, mean_q: 106.559334
 20732/50000: episode: 165, duration: 0.526s, episode steps: 151, steps per second: 287, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.196 [-1.528, 1.076], loss: 10.040893, mean_absolute_error: 53.030262, mean_q: 106.343613
 20894/50000: episode: 166, duration: 0.564s, episode steps: 162, steps per second: 287, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.233 [-1.742, 1.261], loss: 10.266937, mean_absolute_error: 52.995251, mean_q: 106.469284
 21053/50000: episode: 167, duration: 0.554s, episode steps: 159, steps per second: 287, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.255 [-1.683, 1.134], loss: 5.079230, mean_absolute_error: 53.094276, mean_q: 106.726639
 21240/50000: episode: 168, duration: 0.653s, episode steps: 187, steps per second: 286, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.211 [-1.828, 1.133], loss: 3.637107, mean_absolute_error: 53.308029, mean_q: 107.129196
 21362/50000: episode: 169, duration: 0.427s, episode steps: 122, steps per second: 286, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.250 [-1.491, 1.244], loss: 6.557614, mean_absolute_error: 53.026299, mean_q: 106.512924
 21562/50000: episode: 170, duration: 0.693s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.114 [-1.504, 1.155], loss: 7.018776, mean_absolute_error: 53.485283, mean_q: 107.477219
 21762/50000: episode: 171, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.128 [-0.907, 1.277], loss: 5.981405, mean_absolute_error: 53.412628, mean_q: 107.485947
 21962/50000: episode: 172, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.351 [-1.186, 2.342], loss: 9.646163, mean_absolute_error: 53.584549, mean_q: 107.610863
 22162/50000: episode: 173, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.096 [-1.041, 1.182], loss: 9.998112, mean_absolute_error: 53.695900, mean_q: 107.878777
 22362/50000: episode: 174, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.219 [-1.248, 1.534], loss: 12.917156, mean_absolute_error: 53.763985, mean_q: 107.972893
 22558/50000: episode: 175, duration: 0.681s, episode steps: 196, steps per second: 288, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.356 [-1.166, 2.401], loss: 4.845408, mean_absolute_error: 53.797817, mean_q: 108.244835
 22674/50000: episode: 176, duration: 0.405s, episode steps: 116, steps per second: 286, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.194 [-1.591, 1.317], loss: 7.065456, mean_absolute_error: 54.321022, mean_q: 109.235809
 22870/50000: episode: 177, duration: 0.684s, episode steps: 196, steps per second: 287, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.351 [-1.142, 2.404], loss: 8.600467, mean_absolute_error: 53.957043, mean_q: 108.415100
 23070/50000: episode: 178, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.114 [-1.494, 1.313], loss: 6.316566, mean_absolute_error: 54.095341, mean_q: 108.783089
 23270/50000: episode: 179, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.149 [-1.103, 1.317], loss: 8.716813, mean_absolute_error: 54.189716, mean_q: 108.707657
 23419/50000: episode: 180, duration: 0.520s, episode steps: 149, steps per second: 287, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.224 [-1.592, 1.224], loss: 4.342428, mean_absolute_error: 54.166908, mean_q: 108.863365
 23577/50000: episode: 181, duration: 0.551s, episode steps: 158, steps per second: 287, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.162 [-1.556, 1.345], loss: 9.329295, mean_absolute_error: 54.063663, mean_q: 108.518341
 23693/50000: episode: 182, duration: 0.404s, episode steps: 116, steps per second: 287, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.221 [-1.591, 1.203], loss: 4.131718, mean_absolute_error: 54.148113, mean_q: 108.651413
 23846/50000: episode: 183, duration: 0.532s, episode steps: 153, steps per second: 287, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.195 [-1.223, 1.136], loss: 10.675994, mean_absolute_error: 54.276730, mean_q: 108.815422
 23998/50000: episode: 184, duration: 0.529s, episode steps: 152, steps per second: 287, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.265 [-1.894, 1.152], loss: 15.567749, mean_absolute_error: 54.125275, mean_q: 108.299133
 24198/50000: episode: 185, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.228 [-1.156, 1.563], loss: 4.578884, mean_absolute_error: 54.108734, mean_q: 108.604591
 24342/50000: episode: 186, duration: 0.502s, episode steps: 144, steps per second: 287, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.216 [-1.355, 1.215], loss: 6.890851, mean_absolute_error: 53.715042, mean_q: 107.737518
 24472/50000: episode: 187, duration: 0.452s, episode steps: 130, steps per second: 288, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.188 [-1.519, 1.282], loss: 14.366188, mean_absolute_error: 54.064133, mean_q: 108.155060
 24672/50000: episode: 188, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.180 [-1.152, 1.432], loss: 4.842223, mean_absolute_error: 53.762012, mean_q: 107.919548
 24863/50000: episode: 189, duration: 0.663s, episode steps: 191, steps per second: 288, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.370 [-0.872, 2.429], loss: 9.096847, mean_absolute_error: 53.811405, mean_q: 107.799530
 25013/50000: episode: 190, duration: 0.523s, episode steps: 150, steps per second: 287, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.177 [-1.506, 1.147], loss: 8.334414, mean_absolute_error: 53.522270, mean_q: 107.415802
 25213/50000: episode: 191, duration: 0.693s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.223 [-1.141, 1.478], loss: 4.607773, mean_absolute_error: 53.547020, mean_q: 107.500389
 25413/50000: episode: 192, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.197 [-1.028, 1.362], loss: 5.480369, mean_absolute_error: 53.330322, mean_q: 107.151855
 25577/50000: episode: 193, duration: 0.572s, episode steps: 164, steps per second: 287, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.161 [-1.778, 1.287], loss: 6.671150, mean_absolute_error: 53.572361, mean_q: 107.503189
 25723/50000: episode: 194, duration: 0.509s, episode steps: 146, steps per second: 287, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.191 [-1.610, 1.215], loss: 15.278360, mean_absolute_error: 53.368393, mean_q: 106.970825
 25896/50000: episode: 195, duration: 0.601s, episode steps: 173, steps per second: 288, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.205 [-1.485, 1.314], loss: 11.157683, mean_absolute_error: 53.453476, mean_q: 106.953171
 26096/50000: episode: 196, duration: 0.696s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.275 [-1.256, 1.869], loss: 7.572911, mean_absolute_error: 53.193138, mean_q: 106.669456
 26296/50000: episode: 197, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.219 [-1.452, 1.215], loss: 5.529346, mean_absolute_error: 53.196896, mean_q: 106.700623
 26496/50000: episode: 198, duration: 0.693s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.230 [-1.413, 1.565], loss: 6.705355, mean_absolute_error: 53.250443, mean_q: 106.788887
 26696/50000: episode: 199, duration: 0.693s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.209 [-1.279, 1.446], loss: 4.841917, mean_absolute_error: 53.028900, mean_q: 106.440186
 26896/50000: episode: 200, duration: 0.696s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.278 [-1.276, 1.932], loss: 5.298616, mean_absolute_error: 53.152863, mean_q: 106.730980
 27096/50000: episode: 201, duration: 0.702s, episode steps: 200, steps per second: 285, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.357 [-1.263, 2.301], loss: 9.299069, mean_absolute_error: 52.843121, mean_q: 105.967667
 27296/50000: episode: 202, duration: 0.719s, episode steps: 200, steps per second: 278, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.023 [-1.373, 1.309], loss: 3.486176, mean_absolute_error: 53.073765, mean_q: 106.477013
 27496/50000: episode: 203, duration: 0.699s, episode steps: 200, steps per second: 286, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.035 [-1.163, 1.302], loss: 8.085929, mean_absolute_error: 52.933372, mean_q: 106.014534
 27696/50000: episode: 204, duration: 0.703s, episode steps: 200, steps per second: 285, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.347 [-1.076, 2.249], loss: 9.415298, mean_absolute_error: 52.689209, mean_q: 105.407921
 27896/50000: episode: 205, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.181 [-1.677, 1.767], loss: 4.196352, mean_absolute_error: 52.616772, mean_q: 105.490997
 28096/50000: episode: 206, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.198 [-1.384, 1.564], loss: 4.316817, mean_absolute_error: 52.791332, mean_q: 105.778000
 28296/50000: episode: 207, duration: 0.693s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.278 [-1.288, 1.789], loss: 4.136690, mean_absolute_error: 52.599541, mean_q: 105.383789
 28496/50000: episode: 208, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.210 [-1.353, 1.350], loss: 4.093819, mean_absolute_error: 52.598682, mean_q: 105.398750
 28696/50000: episode: 209, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.025 [-1.542, 1.434], loss: 5.671836, mean_absolute_error: 52.275040, mean_q: 104.812164
 28896/50000: episode: 210, duration: 0.696s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.053 [-1.647, 1.359], loss: 2.972665, mean_absolute_error: 52.413841, mean_q: 104.999130
 29096/50000: episode: 211, duration: 0.697s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.124 [-1.582, 1.671], loss: 4.367981, mean_absolute_error: 52.524384, mean_q: 105.236084
 29296/50000: episode: 212, duration: 0.706s, episode steps: 200, steps per second: 283, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.103 [-1.668, 1.672], loss: 6.699500, mean_absolute_error: 52.779190, mean_q: 105.522034
 29496/50000: episode: 213, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.156 [-1.399, 1.362], loss: 7.821219, mean_absolute_error: 52.490288, mean_q: 105.128281
 29696/50000: episode: 214, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.088 [-1.606, 1.535], loss: 3.221158, mean_absolute_error: 52.580597, mean_q: 105.372551
 29896/50000: episode: 215, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.070 [-1.559, 1.617], loss: 10.826047, mean_absolute_error: 52.489609, mean_q: 104.893517
 30096/50000: episode: 216, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.114 [-1.693, 1.532], loss: 6.769707, mean_absolute_error: 52.335796, mean_q: 104.661095
 30296/50000: episode: 217, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.021 [-1.523, 1.614], loss: 8.184737, mean_absolute_error: 52.388653, mean_q: 104.929573
 30496/50000: episode: 218, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.086 [-1.735, 1.432], loss: 4.250251, mean_absolute_error: 52.518784, mean_q: 105.322754
 30696/50000: episode: 219, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.083 [-1.740, 1.637], loss: 5.205548, mean_absolute_error: 52.431553, mean_q: 105.078827
 30896/50000: episode: 220, duration: 0.693s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.086 [-1.624, 1.428], loss: 9.380254, mean_absolute_error: 52.126904, mean_q: 104.318909
 31096/50000: episode: 221, duration: 0.693s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.093 [-1.695, 1.785], loss: 4.880100, mean_absolute_error: 52.259903, mean_q: 104.907341
 31296/50000: episode: 222, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.099 [-1.733, 1.691], loss: 10.694839, mean_absolute_error: 52.018967, mean_q: 104.089287
 31485/50000: episode: 223, duration: 0.658s, episode steps: 189, steps per second: 287, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.242 [-2.235, 1.664], loss: 2.244634, mean_absolute_error: 51.976448, mean_q: 104.244400
 31685/50000: episode: 224, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.052 [-1.443, 1.714], loss: 9.597433, mean_absolute_error: 52.195244, mean_q: 104.278915
 31885/50000: episode: 225, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.001 [-1.619, 1.531], loss: 5.279990, mean_absolute_error: 52.011616, mean_q: 104.231178
 32085/50000: episode: 226, duration: 0.693s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.041 [-1.541, 1.920], loss: 6.796589, mean_absolute_error: 52.135094, mean_q: 104.309105
 32285/50000: episode: 227, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.051 [-1.394, 1.600], loss: 3.052840, mean_absolute_error: 52.006851, mean_q: 104.275726
 32485/50000: episode: 228, duration: 0.693s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.133 [-1.843, 1.706], loss: 5.216931, mean_absolute_error: 51.858585, mean_q: 103.840729
 32685/50000: episode: 229, duration: 0.697s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.238 [-1.654, 1.569], loss: 11.615334, mean_absolute_error: 51.862381, mean_q: 103.667908
 32885/50000: episode: 230, duration: 0.696s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.114 [-1.491, 1.538], loss: 3.805731, mean_absolute_error: 51.703056, mean_q: 103.708191
 33085/50000: episode: 231, duration: 0.693s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.029 [-1.390, 1.274], loss: 9.136116, mean_absolute_error: 51.865582, mean_q: 103.875275
 33285/50000: episode: 232, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.032 [-1.520, 1.793], loss: 5.452350, mean_absolute_error: 51.715397, mean_q: 103.581436
 33485/50000: episode: 233, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.083 [-1.336, 1.209], loss: 12.226831, mean_absolute_error: 51.428642, mean_q: 102.935432
 33685/50000: episode: 234, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.024 [-1.792, 1.554], loss: 5.661469, mean_absolute_error: 51.371826, mean_q: 103.025177
 33885/50000: episode: 235, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.073 [-1.468, 1.515], loss: 5.832118, mean_absolute_error: 51.057892, mean_q: 102.514305
 34085/50000: episode: 236, duration: 0.696s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.067 [-1.400, 1.449], loss: 6.656598, mean_absolute_error: 50.991886, mean_q: 102.183517
 34285/50000: episode: 237, duration: 0.697s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.041 [-1.902, 1.568], loss: 12.718320, mean_absolute_error: 51.365585, mean_q: 102.636520
 34485/50000: episode: 238, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.026 [-1.749, 1.316], loss: 8.170376, mean_absolute_error: 50.908028, mean_q: 101.970001
 34685/50000: episode: 239, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.000 [-1.484, 1.719], loss: 4.045408, mean_absolute_error: 50.792179, mean_q: 101.825623
 34885/50000: episode: 240, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.201 [-1.342, 1.344], loss: 8.093910, mean_absolute_error: 50.662796, mean_q: 101.489899
 35085/50000: episode: 241, duration: 0.693s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.149 [-1.686, 1.737], loss: 6.848406, mean_absolute_error: 50.653915, mean_q: 101.478966
 35285/50000: episode: 242, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.236 [-1.664, 1.664], loss: 8.518634, mean_absolute_error: 50.743763, mean_q: 101.680130
 35485/50000: episode: 243, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.118 [-1.946, 1.765], loss: 6.770204, mean_absolute_error: 50.662216, mean_q: 101.505432
 35685/50000: episode: 244, duration: 0.700s, episode steps: 200, steps per second: 286, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.081 [-1.833, 1.583], loss: 5.403269, mean_absolute_error: 50.322353, mean_q: 101.088264
 35885/50000: episode: 245, duration: 0.693s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.110 [-1.624, 1.934], loss: 6.763118, mean_absolute_error: 50.568653, mean_q: 101.423439
 36085/50000: episode: 246, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.005 [-1.607, 1.702], loss: 7.995779, mean_absolute_error: 50.715641, mean_q: 101.759102
 36285/50000: episode: 247, duration: 0.697s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.027 [-1.466, 1.699], loss: 7.025117, mean_absolute_error: 50.851524, mean_q: 102.135139
 36485/50000: episode: 248, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.001 [-1.533, 1.563], loss: 10.682038, mean_absolute_error: 50.827850, mean_q: 101.939178
 36685/50000: episode: 249, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.030 [-1.303, 1.484], loss: 9.744657, mean_absolute_error: 50.609291, mean_q: 101.532272
 36885/50000: episode: 250, duration: 0.696s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.049 [-1.711, 1.580], loss: 11.020714, mean_absolute_error: 50.756710, mean_q: 101.904182
 37085/50000: episode: 251, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.302 [-1.579, 1.941], loss: 9.343670, mean_absolute_error: 50.624699, mean_q: 101.752350
 37285/50000: episode: 252, duration: 0.696s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.033 [-1.630, 1.449], loss: 13.363928, mean_absolute_error: 50.398407, mean_q: 101.135284
 37485/50000: episode: 253, duration: 0.696s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.144 [-1.619, 1.576], loss: 8.273837, mean_absolute_error: 50.450573, mean_q: 101.414513
 37685/50000: episode: 254, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.134 [-1.400, 1.497], loss: 4.842251, mean_absolute_error: 50.501637, mean_q: 101.583351
 37885/50000: episode: 255, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.097 [-1.558, 1.559], loss: 7.378245, mean_absolute_error: 50.516220, mean_q: 101.693787
 38085/50000: episode: 256, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.143 [-1.750, 1.565], loss: 5.916608, mean_absolute_error: 50.754086, mean_q: 102.048828
 38285/50000: episode: 257, duration: 0.696s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.084 [-1.750, 1.572], loss: 7.410509, mean_absolute_error: 50.951366, mean_q: 102.449142
 38485/50000: episode: 258, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.055 [-1.447, 1.551], loss: 7.797462, mean_absolute_error: 50.926193, mean_q: 102.289024
 38685/50000: episode: 259, duration: 0.697s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.034 [-1.378, 1.419], loss: 6.869923, mean_absolute_error: 50.886768, mean_q: 102.222649
 38885/50000: episode: 260, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.014 [-1.631, 1.639], loss: 12.758966, mean_absolute_error: 50.997326, mean_q: 102.266228
 39085/50000: episode: 261, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.009 [-1.516, 1.509], loss: 12.266820, mean_absolute_error: 50.997025, mean_q: 102.420311
 39285/50000: episode: 262, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.032 [-1.266, 1.208], loss: 8.989419, mean_absolute_error: 51.083946, mean_q: 102.721153
 39485/50000: episode: 263, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.018 [-1.297, 1.371], loss: 6.092255, mean_absolute_error: 50.932594, mean_q: 102.303612
 39685/50000: episode: 264, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.023 [-1.753, 1.485], loss: 9.016157, mean_absolute_error: 51.192471, mean_q: 102.712677
 39885/50000: episode: 265, duration: 0.696s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.023 [-1.531, 1.539], loss: 8.494211, mean_absolute_error: 51.190296, mean_q: 102.849236
 40085/50000: episode: 266, duration: 0.697s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.011 [-1.465, 1.676], loss: 9.428252, mean_absolute_error: 51.394608, mean_q: 103.407043
 40285/50000: episode: 267, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.020 [-1.423, 1.502], loss: 16.208708, mean_absolute_error: 51.645878, mean_q: 103.719376
 40485/50000: episode: 268, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.038 [-1.173, 1.093], loss: 13.559907, mean_absolute_error: 51.389652, mean_q: 103.495354
 40685/50000: episode: 269, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.029 [-1.346, 1.236], loss: 10.885706, mean_absolute_error: 51.827480, mean_q: 104.372971
 40885/50000: episode: 270, duration: 0.697s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.013 [-1.377, 1.009], loss: 9.224025, mean_absolute_error: 51.867954, mean_q: 104.577385
 41085/50000: episode: 271, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.015 [-1.184, 1.382], loss: 10.464860, mean_absolute_error: 51.954971, mean_q: 104.767578
 41285/50000: episode: 272, duration: 0.813s, episode steps: 200, steps per second: 246, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.015 [-1.485, 1.256], loss: 12.164702, mean_absolute_error: 51.898659, mean_q: 104.471428
 41485/50000: episode: 273, duration: 0.705s, episode steps: 200, steps per second: 284, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.009 [-1.226, 1.446], loss: 11.477064, mean_absolute_error: 51.790565, mean_q: 104.329941
 41685/50000: episode: 274, duration: 0.719s, episode steps: 200, steps per second: 278, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.003 [-1.473, 1.230], loss: 12.648816, mean_absolute_error: 52.196217, mean_q: 105.109657
 41885/50000: episode: 275, duration: 0.735s, episode steps: 200, steps per second: 272, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.020 [-1.103, 1.124], loss: 16.815208, mean_absolute_error: 52.200851, mean_q: 104.774216
 42085/50000: episode: 276, duration: 0.711s, episode steps: 200, steps per second: 281, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.074 [-1.357, 1.294], loss: 13.967481, mean_absolute_error: 52.116791, mean_q: 104.833748
 42285/50000: episode: 277, duration: 0.714s, episode steps: 200, steps per second: 280, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.015 [-1.570, 1.711], loss: 7.963239, mean_absolute_error: 52.358856, mean_q: 105.496086
 42485/50000: episode: 278, duration: 0.706s, episode steps: 200, steps per second: 283, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.020 [-1.565, 1.742], loss: 18.785515, mean_absolute_error: 52.497627, mean_q: 105.371292
 42685/50000: episode: 279, duration: 0.704s, episode steps: 200, steps per second: 284, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.021 [-1.453, 1.645], loss: 21.093306, mean_absolute_error: 52.395554, mean_q: 105.126701
 42885/50000: episode: 280, duration: 0.707s, episode steps: 200, steps per second: 283, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.008 [-1.349, 1.555], loss: 8.427252, mean_absolute_error: 52.275143, mean_q: 105.236504
 43085/50000: episode: 281, duration: 0.703s, episode steps: 200, steps per second: 284, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.002 [-1.654, 1.394], loss: 9.588153, mean_absolute_error: 52.590721, mean_q: 105.683640
 43285/50000: episode: 282, duration: 0.749s, episode steps: 200, steps per second: 267, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.002 [-1.853, 1.521], loss: 8.249652, mean_absolute_error: 53.011383, mean_q: 106.554840
 43485/50000: episode: 283, duration: 0.787s, episode steps: 200, steps per second: 254, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.005 [-1.598, 1.784], loss: 16.492373, mean_absolute_error: 53.265732, mean_q: 106.865799
 43685/50000: episode: 284, duration: 0.711s, episode steps: 200, steps per second: 281, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.015 [-1.714, 1.752], loss: 13.911360, mean_absolute_error: 53.091679, mean_q: 106.645844
 43885/50000: episode: 285, duration: 0.705s, episode steps: 200, steps per second: 284, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.013 [-1.391, 1.528], loss: 10.353974, mean_absolute_error: 53.151386, mean_q: 107.132324
 44085/50000: episode: 286, duration: 0.698s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.008 [-1.450, 1.347], loss: 13.667973, mean_absolute_error: 53.749630, mean_q: 108.258064
 44285/50000: episode: 287, duration: 0.696s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.017 [-1.435, 1.431], loss: 15.919949, mean_absolute_error: 53.631729, mean_q: 107.946625
 44485/50000: episode: 288, duration: 0.696s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.003 [-1.491, 1.237], loss: 15.183655, mean_absolute_error: 53.766201, mean_q: 108.112381
 44685/50000: episode: 289, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.009 [-1.633, 1.518], loss: 13.975489, mean_absolute_error: 53.914726, mean_q: 108.599434
 44885/50000: episode: 290, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.025 [-1.355, 1.543], loss: 12.456116, mean_absolute_error: 54.037140, mean_q: 109.116676
 45085/50000: episode: 291, duration: 0.696s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.011 [-1.303, 1.594], loss: 16.036776, mean_absolute_error: 54.019833, mean_q: 108.755348
 45285/50000: episode: 292, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.030 [-1.290, 1.170], loss: 15.197117, mean_absolute_error: 53.983803, mean_q: 108.894470
 45485/50000: episode: 293, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.018 [-1.220, 1.374], loss: 13.153668, mean_absolute_error: 53.981098, mean_q: 108.887245
 45685/50000: episode: 294, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.008 [-1.263, 1.397], loss: 10.378249, mean_absolute_error: 54.314377, mean_q: 109.583878
 45885/50000: episode: 295, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.016 [-1.539, 1.649], loss: 8.188171, mean_absolute_error: 54.472725, mean_q: 109.901764
 46085/50000: episode: 296, duration: 0.700s, episode steps: 200, steps per second: 286, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.023 [-1.462, 1.413], loss: 15.255139, mean_absolute_error: 54.492989, mean_q: 109.882523
 46285/50000: episode: 297, duration: 0.695s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.004 [-1.528, 1.480], loss: 12.663616, mean_absolute_error: 54.648758, mean_q: 110.127159
 46485/50000: episode: 298, duration: 0.708s, episode steps: 200, steps per second: 282, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.000 [-1.619, 1.298], loss: 11.101240, mean_absolute_error: 54.768227, mean_q: 110.320824
 46685/50000: episode: 299, duration: 0.696s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.015 [-1.493, 1.644], loss: 16.579956, mean_absolute_error: 54.888416, mean_q: 110.495918
 46885/50000: episode: 300, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.025 [-1.498, 1.576], loss: 20.385971, mean_absolute_error: 54.696987, mean_q: 109.899300
 47085/50000: episode: 301, duration: 0.701s, episode steps: 200, steps per second: 285, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.016 [-1.484, 1.678], loss: 20.085320, mean_absolute_error: 54.944820, mean_q: 110.591690
 47285/50000: episode: 302, duration: 0.713s, episode steps: 200, steps per second: 281, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.016 [-1.678, 1.722], loss: 17.879185, mean_absolute_error: 54.914162, mean_q: 110.276756
 47485/50000: episode: 303, duration: 0.703s, episode steps: 200, steps per second: 285, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.016 [-1.515, 1.493], loss: 24.173794, mean_absolute_error: 55.210037, mean_q: 110.680626
 47685/50000: episode: 304, duration: 0.701s, episode steps: 200, steps per second: 285, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.023 [-1.486, 1.719], loss: 21.969254, mean_absolute_error: 55.144363, mean_q: 110.753311
 47885/50000: episode: 305, duration: 0.710s, episode steps: 200, steps per second: 282, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.020 [-1.516, 1.643], loss: 17.254801, mean_absolute_error: 55.016758, mean_q: 110.654335
 48085/50000: episode: 306, duration: 0.697s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.011 [-1.698, 1.412], loss: 14.406159, mean_absolute_error: 54.884247, mean_q: 110.350334
 48285/50000: episode: 307, duration: 0.693s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.024 [-1.970, 1.592], loss: 18.192158, mean_absolute_error: 54.814159, mean_q: 110.312477
 48485/50000: episode: 308, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.002 [-1.466, 1.420], loss: 10.807290, mean_absolute_error: 55.214794, mean_q: 111.456833
 48685/50000: episode: 309, duration: 0.700s, episode steps: 200, steps per second: 286, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.007 [-1.606, 1.763], loss: 17.925276, mean_absolute_error: 55.619698, mean_q: 112.043030
 48885/50000: episode: 310, duration: 0.693s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.014 [-1.796, 1.595], loss: 16.196871, mean_absolute_error: 56.338757, mean_q: 113.641014
 49085/50000: episode: 311, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.043 [-1.481, 1.604], loss: 17.336607, mean_absolute_error: 56.294220, mean_q: 113.776749
 49285/50000: episode: 312, duration: 0.693s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.003 [-1.757, 1.429], loss: 20.588438, mean_absolute_error: 56.778862, mean_q: 114.537590
 49485/50000: episode: 313, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.014 [-1.325, 1.414], loss: 16.319138, mean_absolute_error: 57.148262, mean_q: 115.437370
 49685/50000: episode: 314, duration: 0.697s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.039 [-1.309, 1.550], loss: 23.869900, mean_absolute_error: 57.202805, mean_q: 115.464088
 49885/50000: episode: 315, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.016 [-1.314, 1.532], loss: 16.348753, mean_absolute_error: 57.343380, mean_q: 115.738098
done, took 175.217 seconds
visualize_log starting
Exception ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x11140eda0>>
Traceback (most recent call last):
  File "/Users/kimardenmiller/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 532, in __del__
UnboundLocalError: local variable 'status' referenced before assignment
